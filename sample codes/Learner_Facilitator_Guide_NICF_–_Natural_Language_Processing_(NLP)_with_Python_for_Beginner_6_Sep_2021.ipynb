{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Learner/Facilitator Guide - NICF – Natural Language Processing (NLP) with Python for Beginner_6_Sep_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "q57FVc2tVqRb",
        "C0a6eMRpt_6P",
        "1jlox9EnuFoX"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NNddRftQM5s"
      },
      "source": [
        "# NICF – Natural Language Processing (NLP) with Python for Beginners"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Jl-ern-xhP"
      },
      "source": [
        "# Topic 1 Overview of NLP and Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frNQRGpi-xhV"
      },
      "source": [
        "# Topic 2 Language Modeling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBGYbXC5bxsl"
      },
      "source": [
        "## Install spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6RxKM3HALgQ"
      },
      "source": [
        "# Install spaCy v3\n",
        "!pip3 install spacy==3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iZvlwS2-xhU"
      },
      "source": [
        "import spacy\n",
        "print(spacy.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9OLIfjRkJxU"
      },
      "source": [
        "## The nlp object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb-9ROkB-xhV"
      },
      "source": [
        "# Import the English language class\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create the nlp object\n",
        "nlp = English()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpuYqCDqief1"
      },
      "source": [
        "nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5l3RNVikORf"
      },
      "source": [
        "## The Doc object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx4gfd9CkXch"
      },
      "source": [
        "# Created by processing a string of text with the nlp object\n",
        "doc = nlp(\"Hello world!\")\n",
        "doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tusTmj9bLeb"
      },
      "source": [
        "text = open('sample.txt').read()\n",
        "doc = nlp(text)\n",
        "doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aITozwgrmBBx"
      },
      "source": [
        "## The Token object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ic4xZuYmSVD"
      },
      "source": [
        "doc = nlp(\"Hello world!\")\n",
        "doc\n",
        "\n",
        "# Iterate over tokens in a Doc\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbSJQWP4mUrY"
      },
      "source": [
        "# Index into the Doc to get a single Token\n",
        "token = doc[1]\n",
        "\n",
        "# Get the token text via the .text attribute\n",
        "print(token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abMP3mB556hY"
      },
      "source": [
        "### Activity: The Doc and Token Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmjZOFFqB-jt"
      },
      "source": [
        "# Import the English language class\n",
        "from spacy.lang.____ import ____\n",
        "​\n",
        "# Create the nlp object\n",
        "nlp = ____\n",
        "​\n",
        "# Process a text\n",
        "text = open('sample.txt').read()\n",
        "doc = nlp(text)\n",
        "​\n",
        "# Print the document text\n",
        "print(____.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvoeKUUyB_PT"
      },
      "source": [
        "### Solution: The Doc and Token Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQGKI-p455kg"
      },
      "source": [
        "# Import the English language class and create the nlp object\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create the nlp object\n",
        "nlp = English()\n",
        "\n",
        "# Process the text\n",
        "text = open('sample.txt').read()\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print the token's text\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPAMBDJ_CoYQ"
      },
      "source": [
        "### Activity: The Token Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbXvvlGhCtWj"
      },
      "source": [
        "# Import the English language class and create the nlp object\n",
        "from ____ import ____\n",
        "​\n",
        "nlp = ____\n",
        "​\n",
        "# Process the text\n",
        "doc = ____(\"I like tree kangaroos and narwhals.\")\n",
        "​\n",
        "# Select the first token\n",
        "first_token = doc[____]\n",
        "​\n",
        "# Print the first token's text\n",
        "print(first_token.____)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqcWBKBiCwh9"
      },
      "source": [
        "### Solution: The Token Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buwF6M4XCzi8"
      },
      "source": [
        "# Import the English language class and create the nlp object\n",
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
        "\n",
        "# Select the first token\n",
        "first_token = doc[0]\n",
        "\n",
        "# Print the first token's text\n",
        "print(first_token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bXAmiBmCGRU"
      },
      "source": [
        "## Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0jSV4RtdB4c"
      },
      "source": [
        "stopwords = nlp.Defaults.stop_words\n",
        "stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oapWUXZcdVvz"
      },
      "source": [
        "import string\n",
        "punctuations = string.punctuation\n",
        "\n",
        "stopwords = stopwords.union({'\\n'})\n",
        "\n",
        "text = open('sample.txt').read()\n",
        "doc = nlp(text)\n",
        "\n",
        "tokens = [token.text for token in doc if token.text not in stopwords and token.text not in punctuations]\n",
        "cleaned_doc = ' '.join(tokens)\n",
        "cleaned_doc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dag2rlN9erjZ"
      },
      "source": [
        "### Activity: Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3PxEw48eo-5"
      },
      "source": [
        "text= \"Clutching the coin, Maria ran to the shops. She went straight to the counter and bought the sweets\"\n",
        "doc = nlp(text)\n",
        "\n",
        "tokens = [tok.text for tok in doc if tok.text not in _______ and tok.text not in _______]\n",
        "cleaned_doc = ' '.join(___)\n",
        "cleaned_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG8UxVYqHyzU"
      },
      "source": [
        "### Solution: Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW9pMM_dHgcJ"
      },
      "source": [
        "text= \"Clutching the coin, Maria ran to the shops. She went straight to the counter and bought the sweets\"\n",
        "doc = nlp(text)\n",
        "\n",
        "tokens = [tok.text for tok in doc if tok.text not in stopwords and tok.text not in punctuations]\n",
        "cleaned_doc = ' '.join(tokens)\n",
        "cleaned_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAeSowy0oOAF"
      },
      "source": [
        "## The Span object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-0dTqQYoSMV"
      },
      "source": [
        "doc = nlp(\"Hello world!\")\n",
        "\n",
        "# A slice from the Doc is a Span object\n",
        "span = doc[1:3]\n",
        "\n",
        "# Get the span text via the .text attribute\n",
        "print(span.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPSwGSE68vuq"
      },
      "source": [
        "### Activity: The Span Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFb34iHF7y3P"
      },
      "source": [
        "# Import the English language class and create the nlp object\n",
        "from ____ import ____\n",
        "\n",
        "nlp = ____\n",
        "\n",
        "# Process the text\n",
        "doc = ____(\"I like tree kangaroos and narwhals.\")\n",
        "\n",
        "# A slice of the Doc for \"tree kangaroos\"\n",
        "tree_kangaroos = ____\n",
        "print(tree_kangaroos.text)\n",
        "\n",
        "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
        "tree_kangaroos_and_narwhals = ____\n",
        "print(tree_kangaroos_and_narwhals.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIPfkAUCDxFP"
      },
      "source": [
        "### Solution: The Span Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-bY0w6ODz-M"
      },
      "source": [
        "# Import the English language class and create the nlp object\n",
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
        "\n",
        "# A slice of the Doc for \"tree kangaroos\"\n",
        "tree_kangaroos = doc[2:4]\n",
        "print(tree_kangaroos.text)\n",
        "\n",
        "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
        "tree_kangaroos_and_narwhals = doc[2:6]\n",
        "print(tree_kangaroos_and_narwhals.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKetAIDn1wSY"
      },
      "source": [
        "## Lexical Attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B97n9QB_1yWp"
      },
      "source": [
        "doc = nlp(\"It costs $5.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GemwBDYc2Ga9"
      },
      "source": [
        "print(\"Index:   \", [token.i for token in doc])\n",
        "print(\"Text:    \", [token.text for token in doc])\n",
        "\n",
        "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
        "print(\"is_punct:\", [token.is_punct for token in doc])\n",
        "print(\"like_num:\", [token.like_num for token in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1Zzfes3_p1Y"
      },
      "source": [
        "### Activity: Lexical Attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjg-kNSSETmq"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\n",
        "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
        "    \"Now less than 4% are.\"\n",
        ")\n",
        "\n",
        "# Iterate over the tokens in the doc\n",
        "for token in doc:\n",
        "    # Check if the token resembles a number\n",
        "    if ____.____:\n",
        "        # Get the next token in the document\n",
        "        next_token = ____[____]\n",
        "        # Check if the next token's text equals \"%\"\n",
        "        if next_token.____ == \"%\":\n",
        "            print(\"Percentage found:\", token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1GUnklUEU-o"
      },
      "source": [
        "### Solution: Lexical Attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TGTMoyW9Gfg"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\n",
        "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
        "    \"Now less than 4% are.\"\n",
        ")\n",
        "# Iterate over the tokens in the doc\n",
        "for token in doc:\n",
        "    # Check if the token resembles a number\n",
        "    if token.like_num:\n",
        "        # Get the next token in the document\n",
        "        next_token = doc[token.i+1]\n",
        "        # Check if the next token's text equals \"%\"\n",
        "        if next_token.text == \"%\":\n",
        "            print(\"Percentage found:\", token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HI0-W_d34Bs"
      },
      "source": [
        "## Statistical Model Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS_Sm-vM3ncP"
      },
      "source": [
        "!python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyUDLIaT4DGB"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2v6BwxPUvca"
      },
      "source": [
        "### Activity: Model Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5ImmtrnEeU_"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the \"en_core_web_sm\" model\n",
        "nlp = ____\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = ____\n",
        "\n",
        "# Print the document text\n",
        "print(____.____)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NI9Ba4qEgnk"
      },
      "source": [
        "### Soluion; Model Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBhyj83RUkN4"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the \"en_core_web_sm\" model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print the document text\n",
        "print(doc.text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIcq9XhBFodJ"
      },
      "source": [
        "## Predicting Part-of-Speech Tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6ZRR0iPBpJB"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process a text\n",
        "doc = nlp(\"She ate the pizza\")\n",
        "\n",
        "# Iterate over the tokens\n",
        "for token in doc:\n",
        "    # Print the text and the predicted part-of-speech tag\n",
        "    print(token.text, token.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcYxBpU4HMSQ"
      },
      "source": [
        "## Predicting Syntactic Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twxotFDrFuu0"
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_, token.head.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q57FVc2tVqRb"
      },
      "source": [
        "### Activity: Predicting POS and Syntactic Dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5qwQ9heFCuj"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = ____\n",
        "\n",
        "for token in doc:\n",
        "    # Get the token text, part-of-speech tag and dependency label\n",
        "    token_text = ____.____\n",
        "    token_pos = ____.____\n",
        "    token_dep = ____.____\n",
        "    # This is for formatting only\n",
        "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChTwx06fFDQM"
      },
      "source": [
        "### Solution: Predicting POS and Syntactic Dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6D-VBpgVR35"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    # Get the token text, part-of-speech tag and dependency label\n",
        "    token_text = token.text\n",
        "    token_pos = token.pos_\n",
        "    token_dep = token.dep_\n",
        "    # This is for formatting only\n",
        "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p_1mFwyk-6E"
      },
      "source": [
        "### Visualize Syntactic Dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-xjVUDTik_m"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc = nlp('She ate the pizza')\n",
        "\n",
        "html = displacy.render([doc], style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8KzMnA9KGkH"
      },
      "source": [
        "## Predicting Named Entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvSFJFrxJ4qk"
      },
      "source": [
        "# Process a text\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "# Iterate over the predicted entities\n",
        "for ent in doc.ents:\n",
        "    # Print the entity text and its label\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4FDBa89WLw0"
      },
      "source": [
        "### Activity: Predict Named Entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqT2xzDdFQOb"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = ____\n",
        "\n",
        "# Iterate over the predicted entities\n",
        "for ent in ____.____:\n",
        "    # Print the entity text and its label\n",
        "    print(ent.____, ____.____)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0hM1ZeMFRym"
      },
      "source": [
        "### Solution: Predict Named Entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM_mzu24WDH6"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over the predicted entities\n",
        "for ent in doc.ents:\n",
        "    # Print the entity text and its label\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h5C7doYlVxu"
      },
      "source": [
        "### Visualie Named Entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqP6Fnv_iTie"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
        "\n",
        "html = displacy.render([doc], style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ2upc3aiAHg"
      },
      "source": [
        "## Wrong Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MIZ3vdNWtYS"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Upcoming iPhone X release date leaked as Apple reveals pre-orders\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over the entities\n",
        "for ent in doc.ents:\n",
        "    # Print the entity text and label\n",
        "    print(ent.text, ent.label_)\n",
        "\n",
        "# Get the span for \"iPhone X\"\n",
        "iphone_x = doc[1:3]\n",
        "\n",
        "# Print the span text\n",
        "print(\"Missing entity:\", iphone_x.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrRYiLQSmnSg"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWv_v_CQmRGT"
      },
      "source": [
        "doc = nlp('I ran to the clinic with running nose')\n",
        "\n",
        "for token in doc:\n",
        "\tprint(token.text,token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV-4UPiulayz"
      },
      "source": [
        "## Rule-Based Matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xLzOvAFYV13"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Import the Matcher\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load a model and create the nlp object\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize the matcher with the shared vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
        "matcher.add(\"IPHONE_PATTERN\", [pattern])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "719rzBEMYcyB"
      },
      "source": [
        "# Call the matcher on the doc\n",
        "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dki6H9H6o_Hz"
      },
      "source": [
        "pattern = [\n",
        "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
        "    {'POS': 'NOUN'}\n",
        "]\n",
        "\n",
        "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
        "\n",
        "# Initialize the matcher with the shared vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "matcher.add('PETS', [pattern])\n",
        "\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdnEDFsnqrxs"
      },
      "source": [
        "### Activity 1: Rule Based Matching\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei8fqGuSFtcD"
      },
      "source": [
        "import spacy\n",
        "from spacy.____ import _____\n",
        "\n",
        "doc = nlp(\n",
        "    \"After making the iOS update you won't notice a radical system-wide \"\n",
        "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
        "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
        "    \"some tweaks once you delve a little deeper.\"\n",
        ")\n",
        "\n",
        "# Initialize the Matcher with the shared vocabulary\n",
        "matcher = ____(____.____)\n",
        "\n",
        "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
        "pattern = [________]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1UCJR4VFvum"
      },
      "source": [
        "### Solution: Rule Based Matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vwFW_M6qeSY"
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\n",
        "    \"After making the iOS update you won't notice a radical system-wide \"\n",
        "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
        "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
        "    \"some tweaks once you delve a little deeper.\"\n",
        ")\n",
        "\n",
        "# Initialize the Matcher with the shared vocabulary\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
        "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaULpUkAUjzy"
      },
      "source": [
        "### Activity 2 : Rule Based Matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmnKRZdUVHYl"
      },
      "source": [
        "!python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH8dl_6XUpyl"
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = open('unitednations.txt').read()\n",
        "doc = nlp(text)\n",
        "\n",
        "# Initialize the Matcher with the shared vocabulary\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = ________________\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(_________________)\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l92spoJMUqO6"
      },
      "source": [
        "### Solution: Rule Based Matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkc-NxdGUcxm"
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = open('unitednations.txt').read()\n",
        "doc = nlp(text)\n",
        "\n",
        "# Initialize the Matcher with the shared vocabulary\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{'TAG': 'NNP'},\n",
        "           {'TAG': 'NNP'}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"UNITED Nation\", [pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IASdEdSz-xhi"
      },
      "source": [
        "# Topic 3 Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL-Dga8Zs-yY"
      },
      "source": [
        "## Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6IEGT48r43T"
      },
      "source": [
        "# Import the English language class\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create the nlp object\n",
        "nlp = English()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrbX40D8rR-J"
      },
      "source": [
        "doc = nlp(\"I love spicy food\")\n",
        "print(\"hash value:\", nlp.vocab.strings[\"spicy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmdMo7uaYJHX"
      },
      "source": [
        "print(\"string value:\", nlp.vocab.strings[15018391292570830248])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8XbUBwLtPoS"
      },
      "source": [
        "### Activity: Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h27WoGvNtOqg"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "​\n",
        "nlp = English()\n",
        "doc = nlp(\"I have a cat\")\n",
        "​\n",
        "t# Look up the hash for the word \"cat\"\n",
        "cat_hash = ____.____.____[____]\n",
        "print(cat_hash)\n",
        "\n",
        "# Look up the cat_hash to get the string\n",
        "cat_string = ____.____.____[____]\n",
        "print(cat_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl969Zf_tc9X"
      },
      "source": [
        "### Solution: Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TkDUNvBtZr5"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "doc = nlp(\"I have a cat\")\n",
        "\n",
        "# Look up the hash for the word \"cat\"\n",
        "cat_hash = nlp.vocab.strings[\"cat\"]\n",
        "print(cat_hash)\n",
        "\n",
        "# Look up the cat_hash to get the string\n",
        "cat_string = nlp.vocab.strings[cat_hash]\n",
        "print(cat_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0a6eMRpt_6P"
      },
      "source": [
        "### Activity: Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8qDCQySt_TI"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "doc = nlp(\"David Bowie is a PERSON\")\n",
        "\n",
        "# Look up the hash for the string label \"PERSON\"\n",
        "person_hash = ____.____.____[____]\n",
        "print(person_hash)\n",
        "\n",
        "# Look up the person_hash to get the string\n",
        "person_string = ____.____.____[____]\n",
        "print(person_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jlox9EnuFoX"
      },
      "source": [
        "### Solution: Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B5U2HJ1uC9G"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"David Bowie is a PERSON\")\n",
        "\n",
        "# Look up the hash for the string label \"PERSON\"\n",
        "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
        "print(person_hash)\n",
        "\n",
        "# Look up the person_hash to get the string\n",
        "person_string = nlp.vocab.strings[person_hash]\n",
        "print(person_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OzVmVjhtBRP"
      },
      "source": [
        "## Lexeme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WprMyS8as8ii"
      },
      "source": [
        "doc = nlp(\"I love spicy food\")\n",
        "lexeme = nlp.vocab[\"spicy\"]\n",
        "\n",
        "# Print the lexical attributes\n",
        "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMxmaDAqxeSq"
      },
      "source": [
        "## Install Medium English Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-bIktPDxtXC"
      },
      "source": [
        "!python3 -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFS07kIM0IzM"
      },
      "source": [
        "## Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h8gCce9z_Gy"
      },
      "source": [
        "# Load a larger model with vectors\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "doc = nlp(\"I love spciy food\")\n",
        "# Access the vector via the token.vector attribute\n",
        "print(doc[3].vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwoJ33K92emM"
      },
      "source": [
        "### Activity: Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I17ki-ao2cDP"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_md model\n",
        "nlp = ____\n",
        "\n",
        "# Process a text\n",
        "doc = nlp(\"Two bananas in pyjamas\")\n",
        "\n",
        "# Get the vector for the token \"bananas\"\n",
        "bananas_vector = ____.____\n",
        "print(bananas_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI-0tkuh2jKK"
      },
      "source": [
        "### Solution: Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0ql0qx-2mcx"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_md model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Process a text\n",
        "doc = nlp(\"Two bananas in pyjamas\")\n",
        "\n",
        "# Get the vector for the token \"bananas\"\n",
        "bananas_vector = doc[1].vector\n",
        "print(bananas_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cILnVLnqy2zo"
      },
      "source": [
        "## Comparing Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anj45wy4xdAY"
      },
      "source": [
        "# Load a larger model with vectors\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Compare two documents\n",
        "doc1 = nlp(\"I like fast food\")\n",
        "doc2 = nlp(\"I like pizza\")\n",
        "print(doc1.similarity(doc2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv-9yi7cyGdY"
      },
      "source": [
        "# Compare two tokens\n",
        "doc = nlp(\"I like pizza and pasta\")\n",
        "token1 = doc[2]\n",
        "token2 = doc[4]\n",
        "print(token1.similarity(token2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USsw4r7P_taw"
      },
      "source": [
        "dog = nlp.vocab['dog']\n",
        "animal = nlp.vocab['animal']\n",
        "\n",
        "print(dog.similarity(animal))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QJcvLRm_v38"
      },
      "source": [
        "doc1 = nlp(\"Dogs are awesome.\")\n",
        "doc2 = nlp(\"Some gorgeous creatures are felines.\")\n",
        "\n",
        "print(doc1.similarity(doc2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSE_5Zp73MO9"
      },
      "source": [
        "### Activity: Comparing Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vgNPW7U3PRj"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "doc1 = nlp(\"It's a warm summer day\")\n",
        "doc2 = nlp(\"It's sunny outside\")\n",
        "doc3 = nlp(\"I need to take care of my kids today\")\n",
        "\n",
        "# Get the similarity of doc1, doc2 and doc3\n",
        "similarity = ____.____(____)\n",
        "print(similarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoZiMO5f6twZ"
      },
      "source": [
        "### Solution; Comparing Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZQic3mB6qv8"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "doc1 = nlp(\"It's a warm summer day\")\n",
        "doc2 = nlp(\"It's sunny outside\")\n",
        "\n",
        "# Get the similarity of doc1 and doc2\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(similarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru1FyEi3ukox"
      },
      "source": [
        "# Compare a document with a token\n",
        "doc = nlp(\"I like pizza\")\n",
        "token = nlp(\"soap\")[0]\n",
        "\n",
        "print(doc.similarity(token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSd9Qlm9yxcf"
      },
      "source": [
        "# Compare a span with a document\n",
        "span = nlp(\"I like pizza and pasta\")[2:5]\n",
        "doc = nlp(\"McDonalds sells burgers\")\n",
        "\n",
        "print(span.similarity(doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8RUD-ZV8fFU"
      },
      "source": [
        "### Activity: Comparing Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhsh0ZhL8ilF"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "doc = nlp(\"TV and books\")\n",
        "token1, token2 = doc[0], doc[2]\n",
        "\n",
        "# Get the similarity of the tokens \"TV\" and \"books\"\n",
        "similarity = ____.____(____)\n",
        "print(similarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSr072Y28mp2"
      },
      "source": [
        "### Solution: Comparing Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_foRp3i-8rUw"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "doc = nlp(\"TV and books\")\n",
        "token1, token2 = doc[0], doc[2]\n",
        "\n",
        "# Get the similarity of the tokens \"TV\" and \"books\"\n",
        "similarity = token1.similarity(token2)\n",
        "print(similarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfpwqq7x1ioJ"
      },
      "source": [
        "## Similarity Depends on the Application Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBzxpoO61ke4"
      },
      "source": [
        "doc1 = nlp(\"I like cats\")\n",
        "doc2 = nlp(\"I hate cats\")\n",
        "\n",
        "print(doc1.similarity(doc2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxETIjjK86Ml"
      },
      "source": [
        "### Activity: Comparing Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytu2ibXx8-ka"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
        "\n",
        "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
        "span1 = ____\n",
        "span2 = ____\n",
        "\n",
        "# Get the similarity of the spans\n",
        "similarity = ____.____(____)\n",
        "print(similarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXUwLNNH8_yr"
      },
      "source": [
        "### Solution: Comparing Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8EOJRvr9EJr"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
        "\n",
        "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
        "span1 = doc[3:5]\n",
        "span2 = doc[12:15]\n",
        "\n",
        "# Get the similarity of the spans\n",
        "similarity = span1.similarity(span2)\n",
        "print(similarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvYKHJuO-xhf"
      },
      "source": [
        "# Topic 4 Text Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEsETmmQ-xhg"
      },
      "source": [
        "## Default Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4cI0Q9W-xhg"
      },
      "source": [
        "nlp.pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hhUNgkWBD-4"
      },
      "source": [
        "pipeline = [\"tagger\",\"ner\"]\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "for name in pipeline:\n",
        "    nlp.add_pipe(name)                 \n",
        "\n",
        "nlp.pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52dV4ksF-xhg"
      },
      "source": [
        "nlp.pipe_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGSw8Q3oPzt4"
      },
      "source": [
        "### Activity: Inspecting the Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmHrPKhrQrKA"
      },
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = ____\n",
        "\n",
        "# Print the names of the pipeline components\n",
        "print(____.____)\n",
        "\n",
        "# Print the full pipeline of (name, component) tuples\n",
        "print(____.____)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu_ik0TQQuIW"
      },
      "source": [
        "### Solution: Inspecting the Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Glb7VuVjP2xd"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Print the names of the pipeline components\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "# Print the full pipeline of (name, component) tuples\n",
        "print(nlp.pipeline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDdz2wQM-xhg"
      },
      "source": [
        "## Disable Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1jsdB0p-xhg"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load(\"en_core_web_sm\",disable=['parser', 'tagger'])\n",
        "\n",
        "nlp.pipeline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-nH-8qgRr20"
      },
      "source": [
        "doc = nlp(u'The cat sit on the mat')\n",
        "for token in doc:\n",
        "    print(token.text,token.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ8xIS8Y-xhh"
      },
      "source": [
        "## Custom Component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztnwWytBeyXK"
      },
      "source": [
        "### Example 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6DZRz1QuNfM"
      },
      "source": [
        "from spacy.language import Language\n",
        "\n",
        "@Language.component(\"custom_component1\")\n",
        "def custom_component1(doc):\n",
        "    print(\"Doc length:\", len(doc))\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(\"custom_component1\", name=\"custom_component1\", first=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzPFQI-CaJwe"
      },
      "source": [
        "print(\"Pipeline:\", nlp.pipe_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_ujf2WDaLIM"
      },
      "source": [
        "doc = nlp(\"I love spicy food\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0WKmiiVe0Mo"
      },
      "source": [
        "### Example 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7u--Wxfud5N"
      },
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "@Language.component(\"custom_component2\")\n",
        "def custom_component2(doc):\n",
        "    if len(doc) < 10:\n",
        "        print(\"This is a pretty short document.\")\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(\"custom_component2\", name=\"custom_component2\", last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qnljIRpakLO"
      },
      "source": [
        "print(\"Pipeline:\", nlp.pipe_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItNaATPaamBx"
      },
      "source": [
        "doc = nlp(\"I love spicy food\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bn9nNg7iKbJ"
      },
      "source": [
        "### Activity: Custom Component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPgMs5oIU_LR"
      },
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "# Define the custom component\n",
        "@Language.component(\"length_component\")\n",
        "def length_component(doc):\n",
        "    # Get the doc's length\n",
        "    doc_length = ____\n",
        "    print(f\"This document is {doc_length} tokens long.\")\n",
        "    # Return the doc\n",
        "    ____\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add the component after the tokenizer in the pipeline and print the pipe names\n",
        "____.____(____)\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "# Process a text\n",
        "doc = ____\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R30oEMFiyc0"
      },
      "source": [
        "### Solution: Custom Component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4yMn8iTiXOP"
      },
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "# Define the custom component\n",
        "@Language.component(\"length_component\")\n",
        "def length_component(doc):\n",
        "    doc_length = len(doc)\n",
        "    print(f\"This document is {doc_length} tokens long.\")\n",
        "    return doc\n",
        "\n",
        "# Add the component first in the pipeline and print the pipe names\n",
        "nlp.add_pipe(\"length_component\", name=\"length_component\",first=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6le0Pmpja6Wd"
      },
      "source": [
        "print(\"Pipeline:\", nlp.pipe_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHVlWvjea9Lu"
      },
      "source": [
        "doc = nlp(\"I love spicy food\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b7SeAaj-xhh"
      },
      "source": [
        "## Machine Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0oBPJzqcG0h"
      },
      "source": [
        "#### Training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHjhinCtcZkZ"
      },
      "source": [
        "# Upload gadgets.json to\n",
        "# [\n",
        "#     [\"How to preorder the iPhone X\", { \"entities\": [[20, 28, \"GADGET\"]] }],\n",
        "#     [\"iPhone X is coming\", { \"entities\": [[0, 8, \"GADGET\"]] }],\n",
        "#     [\"Should I pay $1,000 for the iPhone X?\", { \"entities\": [[28, 36, \"GADGET\"]] }],\n",
        "#     [\"The iPhone 8 reviews are here\", { \"entities\": [[4, 12, \"GADGET\"]] }],\n",
        "#     [\"Your iPhone goes up to 11 today\", { \"entities\": [[5, 11, \"GADGET\"]] }],\n",
        "#     [\"I need a new phone! Any tips?\", { \"entities\": [] }]\n",
        "# ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrpZNyTmctKM"
      },
      "source": [
        "#### Original NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PU6DP3ccqYD"
      },
      "source": [
        "doc = nlp(\"I want to buy a new iPhone in Indonesia\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIdhupLtcxIp"
      },
      "source": [
        "#### Train New NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL7zAD0b2cl6"
      },
      "source": [
        "import spacy\n",
        "import random\n",
        "import json\n",
        "from spacy.training import Example\n",
        "\n",
        "with open(\"gadgets.json\", encoding=\"utf8\") as f:\n",
        "    TRAINING_DATA = json.loads(f.read())\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "ner.add_label(\"GADGET\")\n",
        "\n",
        "# Start the training\n",
        "nlp.begin_training()\n",
        "examples = []\n",
        "for text, annots in TRAINING_DATA:\n",
        "    examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
        "\n",
        "nlp.initialize(lambda: examples)\n",
        "\n",
        "for i in range(10):\n",
        "    random.shuffle(examples)\n",
        "    for batch in spacy.util.minibatch(examples, size=3):\n",
        "        nlp.update(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyS1TQ4s6UBd"
      },
      "source": [
        "#### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbRrrkM933Ya"
      },
      "source": [
        "doc = nlp(\"I want to buy a new iPhone in Indonesia\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXCQvEbrxRhE"
      },
      "source": [
        "## Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE_AGLLVcG0h"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEGGnockjtmC"
      },
      "source": [
        "# Import pandas & read csv file\n",
        "import pandas as pd\n",
        "reviews=pd.read_csv(\"https://raw.githubusercontent.com/hanzhang0420/Women-Clothing-E-commerce/master/Womens%20Clothing%20E-Commerce%20Reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td4dSZ5xjwe6"
      },
      "source": [
        "# Extract desired columns and view the dataframe \n",
        "reviews = reviews[['Review Text','Recommended IND']].dropna()\n",
        "reviaews.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZhScqiOcG0i"
      },
      "source": [
        "### Add Textcat Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQxmxbX-jz81"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "textcat = nlp.add_pipe(\"textcat\")\n",
        "textcat.add_label(\"POSITIVE\")\n",
        "textcat.add_label(\"NEGATIVE\")\n",
        "\n",
        "nlp.pipe_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81hvkYlkcG0i"
      },
      "source": [
        "### Preparing Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DjR2JIuk5rM"
      },
      "source": [
        "# Converting the dataframe into a list of tuples\n",
        "reviews['tuples'] = reviews.apply(lambda row: (row['Review Text'],row['Recommended IND']), axis=1)\n",
        "train =reviews['tuples'].tolist()\n",
        "train[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj-4zHWHk-10"
      },
      "source": [
        "import random\n",
        "\n",
        "def load_data(limit=0, split=0.8):\n",
        "    train_data=train\n",
        "    # Shuffle the data\n",
        "    random.shuffle(train_data)\n",
        "    texts, labels = zip(*train_data)\n",
        "    # get the categories for each review\n",
        "    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n",
        "\n",
        "    # Splitting the training and evaluation data\n",
        "    split = int(len(train_data) * split)\n",
        "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
        "\n",
        "n_texts=23486\n",
        "\n",
        "# Calling the load_data() function \n",
        "(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n",
        "\n",
        "# Processing the final format of training data\n",
        "train_data = list(zip(train_texts,[{'cats': cats} for cats in train_cats]))\n",
        "train_data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXjsGIPgcG0i"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHhHIm20EqMz"
      },
      "source": [
        "from spacy.training import Example\n",
        "\n",
        "nlp.begin_training()\n",
        "\n",
        "examples = []\n",
        "for text, annots in train_data:\n",
        "    examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
        "\n",
        "nlp.initialize(lambda: examples)\n",
        "\n",
        "for i in range(10):\n",
        "    random.shuffle(examples)\n",
        "    for batch in spacy.util.minibatch(examples, size=3):\n",
        "        nlp.update(batch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ5EQ2_5cG0j"
      },
      "source": [
        "### Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJT1fQGalhh-"
      },
      "source": [
        "test_text=\"I hate this dress\"\n",
        "doc=nlp(test_text)\n",
        "doc.cats "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hioyIcKB-xhm"
      },
      "source": [
        "# Topic 5 Overview of Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeAEjpuQsIEg"
      },
      "source": [
        "# Install spaCy v3\n",
        "!pip3 install spacy==3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqsbYsMaIQhF"
      },
      "source": [
        "!pip3 install spacy-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER14CEQwYGct"
      },
      "source": [
        "!python3 -m spacy download en_core_web_trf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TkB3akPeskn"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "print(nlp.pipe_names) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79oChtWU-xhp"
      },
      "source": [
        "doc = nlp(\"I love spicy Indonesia food\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGr5kV_MeGqn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
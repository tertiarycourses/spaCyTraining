{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NICF – Natural Language Processing (NLP) with Python for Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4Jl-ern-xhP"
   },
   "source": [
    "## Topic 1 Overview of NLP and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frNQRGpi-xhV"
   },
   "source": [
    "## Topic 2 Language Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBGYbXC5bxsl"
   },
   "source": [
    "### Install spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6RxKM3HALgQ"
   },
   "outputs": [],
   "source": [
    "#Install spaCy v2\n",
    "#!pip3 install spacy=2.2.4\n",
    "\n",
    "#Install spaCy v3\n",
    "#!pip3 install spacy==3.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_iZvlwS2-xhU",
    "outputId": "8992a09b-7501-4bbd-ba05-f3727576ecd4"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9OLIfjRkJxU"
   },
   "source": [
    "### The nlp object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xb-9ROkB-xhV"
   },
   "outputs": [],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpuYqCDqief1"
   },
   "outputs": [],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5l3RNVikORf"
   },
   "source": [
    "### The Doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gx4gfd9CkXch"
   },
   "outputs": [],
   "source": [
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp(\"Hello world!\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tusTmj9bLeb"
   },
   "outputs": [],
   "source": [
    "text = open('sample.txt').read()\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aITozwgrmBBx"
   },
   "source": [
    "### The Token object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ic4xZuYmSVD"
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Hello world!\")\n",
    "doc\n",
    "\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbSJQWP4mUrY"
   },
   "outputs": [],
   "source": [
    "# Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "\n",
    "# Get the token text via the .text attribute\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abMP3mB556hY"
   },
   "source": [
    "#### Activity: The Doc and Token Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmjZOFFqB-jt"
   },
   "outputs": [],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.____ import ____\n",
    "​\n",
    "# Create the nlp object\n",
    "nlp = ____\n",
    "​\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "​\n",
    "# Print the document text\n",
    "print(____.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvoeKUUyB_PT"
   },
   "source": [
    "#### Solution: The Doc and Token Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQGKI-p455kg"
   },
   "outputs": [],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "# Print the token's text\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPAMBDJ_CoYQ"
   },
   "source": [
    "#### Activity: The Token Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbXvvlGhCtWj"
   },
   "outputs": [],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from ____ import ____\n",
    "​\n",
    "nlp = ____\n",
    "​\n",
    "# Process the text\n",
    "doc = ____(\"I like tree kangaroos and narwhals.\")\n",
    "​\n",
    "# Select the first token\n",
    "first_token = doc[____]\n",
    "​\n",
    "# Print the first token's text\n",
    "print(first_token.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqcWBKBiCwh9"
   },
   "source": [
    "#### Solution: The Token Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buwF6M4XCzi8"
   },
   "outputs": [],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# Select the first token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Print the first token's text\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bXAmiBmCGRU"
   },
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0jSV4RtdB4c"
   },
   "outputs": [],
   "source": [
    "stopwords = nlp.Defaults.stop_words\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oapWUXZcdVvz"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "text = open('sample.txt').read()\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc if token.text not in stopwords and token.text not in punctuations]\n",
    "cleaned_doc = ' '.join(tokens)\n",
    "cleaned_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dag2rlN9erjZ"
   },
   "source": [
    "#### Activity: Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3PxEw48eo-5"
   },
   "outputs": [],
   "source": [
    "text= \"Clutching the coin, Maria ran to the shops. She went straight to the counter and bought the sweets\"\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [tok.text for tok in doc if tok.text not in stopwords and tok.text not in punctuations]\n",
    "cleaned_doc = ' '.join(tokens)\n",
    "cleaned_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAeSowy0oOAF"
   },
   "source": [
    "### The Span object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-0dTqQYoSMV"
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# A slice from the Doc is a Span object\n",
    "span = doc[1:3]\n",
    "\n",
    "# Get the span text via the .text attribute\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPSwGSE68vuq"
   },
   "source": [
    "#### Activity: The Span Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFb34iHF7y3P"
   },
   "outputs": [],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from ____ import ____\n",
    "\n",
    "nlp = ____\n",
    "\n",
    "# Process the text\n",
    "doc = ____(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos\"\n",
    "tree_kangaroos = ____\n",
    "print(tree_kangaroos.text)\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
    "tree_kangaroos_and_narwhals = ____\n",
    "print(tree_kangaroos_and_narwhals.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIPfkAUCDxFP"
   },
   "source": [
    "#### Solution: The Span Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-bY0w6ODz-M"
   },
   "outputs": [],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos\"\n",
    "tree_kangaroos = doc[2:4]\n",
    "print(tree_kangaroos.text)\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
    "tree_kangaroos_and_narwhals = doc[2:6]\n",
    "print(tree_kangaroos_and_narwhals.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKetAIDn1wSY"
   },
   "source": [
    "### Lexical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B97n9QB_1yWp"
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"It costs $5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GemwBDYc2Ga9"
   },
   "outputs": [],
   "source": [
    "print(\"Index:   \", [token.i for token in doc])\n",
    "print(\"Text:    \", [token.text for token in doc])\n",
    "\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1Zzfes3_p1Y"
   },
   "source": [
    "#### Activity: Lexical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjg-kNSSETmq"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if ____.____:\n",
    "        # Get the next token in the document\n",
    "        next_token = ____[____]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.____ == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1GUnklUEU-o"
   },
   "source": [
    "#### Solution: Lexical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TGTMoyW9Gfg"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i+1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HI0-W_d34Bs"
   },
   "source": [
    "### Statistical Model Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SS_Sm-vM3ncP"
   },
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyUDLIaT4DGB"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2v6BwxPUvca"
   },
   "source": [
    "#### Activity: Model Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5ImmtrnEeU_"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the \"en_core_web_sm\" model\n",
    "nlp = ____\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = ____\n",
    "\n",
    "# Print the document text\n",
    "print(____.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NI9Ba4qEgnk"
   },
   "source": [
    "#### Solution; Model Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBhyj83RUkN4"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the \"en_core_web_sm\" model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIcq9XhBFodJ"
   },
   "source": [
    "### Predicting Part-of-Speech Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6ZRR0iPBpJB"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcYxBpU4HMSQ"
   },
   "source": [
    "### Predicting Syntactic Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twxotFDrFuu0"
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q57FVc2tVqRb"
   },
   "source": [
    "#### Activity: Predicting POS and Syntactic Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5qwQ9heFCuj"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = ____\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = ____.____\n",
    "    token_pos = ____.____\n",
    "    token_dep = ____.____\n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChTwx06fFDQM"
   },
   "source": [
    "#### Solution: Predicting POS and Syntactic Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6D-VBpgVR35"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p_1mFwyk-6E"
   },
   "source": [
    "### Visualize Syntactic Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-xjVUDTik_m"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp('She ate the pizza')\n",
    "\n",
    "html = displacy.render([doc], style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8KzMnA9KGkH"
   },
   "source": [
    "### Predicting Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvSFJFrxJ4qk"
   },
   "outputs": [],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4FDBa89WLw0"
   },
   "source": [
    "#### Activity: Predict Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqT2xzDdFQOb"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = ____\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in ____.____:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.____, ____.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0hM1ZeMFRym"
   },
   "source": [
    "#### Solution: Predict Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LM_mzu24WDH6"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_h5C7doYlVxu"
   },
   "source": [
    "### Visualie Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqP6Fnv_iTie"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "html = displacy.render([doc], style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZ2upc3aiAHg"
   },
   "source": [
    "### Wrong Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MIZ3vdNWtYS"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Upcoming iPhone X release date leaked as Apple reveals pre-orders\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Get the span for \"iPhone X\"\n",
    "iphone_x = doc[1:3]\n",
    "\n",
    "# Print the span text\n",
    "print(\"Missing entity:\", iphone_x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrRYiLQSmnSg"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWv_v_CQmRGT"
   },
   "outputs": [],
   "source": [
    "doc = nlp('I ran to the clinic with running nose')\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "\tprint(token.text,token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AV-4UPiulayz"
   },
   "source": [
    "### Rule-Based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xLzOvAFYV13"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "719rzBEMYcyB"
   },
   "outputs": [],
   "source": [
    "# Call the matcher on the doc\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dki6H9H6o_Hz"
   },
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('PETS', [pattern])\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdnEDFsnqrxs"
   },
   "source": [
    "#### Activity: Rule Based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ei8fqGuSFtcD"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.____ import ____\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked as Apple reveals pre-orders\")\n",
    "\n",
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = ____(____.____)\n",
    "\n",
    "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
    "pattern = [____]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "____.____(\"IPHONE_X_PATTERN\",  ____)\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = ____\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1UCJR4VFvum"
   },
   "source": [
    "#### Solutoin: Rule Based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vwFW_M6qeSY"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"After making the iOS update you won't notice a radical system-wide \"\n",
    "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
    "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
    "    \"some tweaks once you delve a little deeper.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IASdEdSz-xhi"
   },
   "source": [
    "## Topic 3 Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wL-Dga8Zs-yY"
   },
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6IEGT48r43T"
   },
   "outputs": [],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrbX40D8rR-J"
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value:\", nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8XbUBwLtPoS"
   },
   "source": [
    "#### Activity: Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h27WoGvNtOqg"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "​\n",
    "nlp = English()\n",
    "doc = nlp(\"I have a cat\")\n",
    "​\n",
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = ____.____.____[____]\n",
    "print(cat_hash)\n",
    "​\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string = ____.____.____[____]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl969Zf_tc9X"
   },
   "source": [
    "#### Solution: Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TkDUNvBtZr5"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0a6eMRpt_6P"
   },
   "source": [
    "#### Activity: Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8qDCQySt_TI"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = ____.____.____[____]\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = ____.____.____[____]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jlox9EnuFoX"
   },
   "source": [
    "#### Solution: Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5B5U2HJ1uC9G"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OzVmVjhtBRP"
   },
   "source": [
    "### Lexeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WprMyS8as8ii"
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMxmaDAqxeSq"
   },
   "source": [
    "### Install Medium English Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-bIktPDxtXC"
   },
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFS07kIM0IzM"
   },
   "source": [
    "### Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5h8gCce9z_Gy"
   },
   "outputs": [],
   "source": [
    "# Load a larger model with vectors\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"I have a banana\")\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwoJ33K92emM"
   },
   "source": [
    "#### Activity: Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I17ki-ao2cDP"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_md model\n",
    "nlp = ____\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = ____.____\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI-0tkuh2jKK"
   },
   "source": [
    "#### Solution: Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0ql0qx-2mcx"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_md model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cILnVLnqy2zo"
   },
   "source": [
    "### Comparing Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Anj45wy4xdAY"
   },
   "outputs": [],
   "source": [
    "# Load a larger model with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pv-9yi7cyGdY"
   },
   "outputs": [],
   "source": [
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USsw4r7P_taw"
   },
   "outputs": [],
   "source": [
    "dog = nlp.vocab['mother']\n",
    "animal = nlp.vocab['father']\n",
    "\n",
    "print(dog.similarity(animal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QJcvLRm_v38"
   },
   "outputs": [],
   "source": [
    "doc1 = nlp(\"Dogs are awesome.\")\n",
    "doc2 = nlp(\"Some gorgeous creatures are felines.\")\n",
    "\n",
    "print(doc1.similarity(doc2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSE_5Zp73MO9"
   },
   "source": [
    "#### Activity: Comparing Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vgNPW7U3PRj"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = ____.____(____)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoZiMO5f6twZ"
   },
   "source": [
    "#### Solution; Comparing Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZQic3mB6qv8"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ru1FyEi3ukox"
   },
   "outputs": [],
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSd9Qlm9yxcf"
   },
   "outputs": [],
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8RUD-ZV8fFU"
   },
   "source": [
    "#### Activity: Comparing Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhsh0ZhL8ilF"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\"\n",
    "similarity = ____.____(____)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSr072Y28mp2"
   },
   "source": [
    "#### Solution: Comparing Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_foRp3i-8rUw"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vfpwqq7x1ioJ"
   },
   "source": [
    "### Similarity Depends on the Application Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBzxpoO61ke4"
   },
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxETIjjK86Ml"
   },
   "source": [
    "#### Activity: Comparing Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ytu2ibXx8-ka"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = ____\n",
    "span2 = ____\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = ____.____(____)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXUwLNNH8_yr"
   },
   "source": [
    "#### Solution: Comparing Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8EOJRvr9EJr"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvYKHJuO-xhf"
   },
   "source": [
    "## Topic 4 Text Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEsETmmQ-xhg"
   },
   "source": [
    "### Default Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4cI0Q9W-xhg"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"I ate apple\"\n",
    "doc = nlp(\"I ate apple\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hhUNgkWBD-4"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\",exclude=['tagger'])\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"I ate apple\"\n",
    "doc = nlp(\"I ate apple\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52dV4ksF-xhg"
   },
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGSw8Q3oPzt4"
   },
   "source": [
    "#### Activity: Inspecting the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmHrPKhrQrKA"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = ____\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(____.____)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(____.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lu_ik0TQQuIW"
   },
   "source": [
    "#### Solution: Inspecting the Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Glb7VuVjP2xd"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDdz2wQM-xhg"
   },
   "source": [
    "### Disable Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1jsdB0p-xhg"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_sm\",disable=['tagger'])\n",
    "\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-nH-8qgRr20"
   },
   "outputs": [],
   "source": [
    "doc = nlp('The cat sit on the mat')\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\",exclude=['tagger'])\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('The cat sit on the mat')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.remove_pipe('tagger')\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('The cat sit on the mat')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ8xIS8Y-xhh"
   },
   "source": [
    "### Custom Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztnwWytBeyXK"
   },
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78QQkzhruWGP"
   },
   "source": [
    "#### SpaCy v2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGUA3bVvd5Y5"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "def custom_component(doc):\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "doc = nlp(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmAYWmMpuaxb"
   },
   "source": [
    "#### SpaCy v3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6DZRz1QuNfM"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component(doc):\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"custom_component\", name=\"custom_component\", first=True)\n",
    "\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "doc = nlp(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0WKmiiVe0Mo"
   },
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxOENJJ9up-K"
   },
   "source": [
    "#### SpaCy v2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyiz2pBZXRYD"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "def my_component(doc):\n",
    "    print(f\"After tokenization, this doc has {len(doc)} tokens.\")\n",
    "    print(\"The part-of-speech tags are:\", [token.pos_ for token in doc])\n",
    "    if len(doc) < 10:\n",
    "        print(\"This is a pretty short document.\")\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(my_component, last=True)\n",
    "print(nlp.pipe_names)  \n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJTd0lnKuwp-"
   },
   "source": [
    "#### SpaCy v3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7u--Wxfud5N"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"info_component\")\n",
    "def my_component(doc):\n",
    "    print(f\"After tokenization, this doc has {len(doc)} tokens.\")\n",
    "    print(\"The part-of-speech tags are:\", [token.pos_ for token in doc])\n",
    "    if len(doc) < 10:\n",
    "        print(\"This is a pretty short document.\")\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"info_component\", name=\"print_info\", last=True)\n",
    "print(nlp.pipe_names)  \n",
    "doc = nlp(\"The fox jump over a lazy dog and a cat chase after a slow rat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Bn9nNg7iKbJ"
   },
   "source": [
    "#### Activity: Custom Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIhZ1oy0vIlS"
   },
   "source": [
    "#### SpaCy v2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lL5-ea_Du3Ea"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = ____\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    ____\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "____.____(____)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT0lbMlNvB5t"
   },
   "source": [
    "#### SpaCy v3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPgMs5oIU_LR"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"length_component\")\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = ____\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    ____\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "____.____(____)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R30oEMFiyc0"
   },
   "source": [
    "#### Solution: Custom Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBLqprAbvMnm"
   },
   "source": [
    "#### SpaCy v2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMb23YC5u774"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(length_component,first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXxUfclAvQC7"
   },
   "source": [
    "#### SpaCy v3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4yMn8iTiXOP"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"length_component\")\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"length_component\", name=\"length_component\",first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 2: Custom Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"my_component2\")\n",
    "def my_component2(doc):\n",
    "    [Add your code here]\n",
    "    return doc\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"____\", name=\"\",last=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution: Custom Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"my_component2\")\n",
    "def my_component2(doc):\n",
    "    for tok in doc:\n",
    "        print(tok.text, tok.pos_)\n",
    "    return doc\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"my_component2\", name=\"my_component2\",last=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 3: Custom Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"my_component3\")\n",
    "def my_component3(doc):\n",
    "    pattern = [\n",
    "    ____\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add('PETS', [pattern])\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        matched_span = doc[start:end]\n",
    "        print(matched_span.text)\n",
    "    return doc\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"___\", name=\"_____\",last=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 3: Custom Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"my_component3\")\n",
    "def my_component3(doc):\n",
    "    pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
    "    {'POS': 'NOUN'}\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add('PETS', [pattern])\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        matched_span = doc[start:end]\n",
    "        print(matched_span.text)\n",
    "    return doc\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"my_component3\", name=\"my_component3\",last=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b7SeAaj-xhh"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SL7zAD0b2cl6"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import json\n",
    "from spacy.training import Example\n",
    "\n",
    "with open(\"gadgets.json\", encoding=\"utf8\") as f:\n",
    "    TRAINING_DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe('ner')\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "examples = []\n",
    "for text, annots in TRAINING_DATA:\n",
    "    examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "    \n",
    "#print(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train th Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.begin_training()\n",
    "for i in range(10):\n",
    "    random.shuffle(examples)\n",
    "    for batch in spacy.util.minibatch(examples, size=3):\n",
    "        nlp.update(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyS1TQ4s6UBd"
   },
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbRrrkM933Ya"
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"I want to buy a new iPhone 12 and iPhone X this Christmas\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp('I want to buy a new iPhone 12 and iPhone X this Christmas')\n",
    "\n",
    "html = displacy.render([doc], style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classifcation (Example 1)\n",
    "####  The example below only works for SpaCy v3.x (Error due to compatibility issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# tqdm is a great progress bar for python\n",
    "# tqdm.auto automatically selects a text based progress \n",
    "# for the console \n",
    "# and html based output in jupyter notebooks\n",
    "from tqdm.auto import tqdm\n",
    "# DocBin is spacys new way to store Docs in a \n",
    "# binary format for training later\n",
    "from spacy.tokens import DocBin\n",
    "# We want to classify movie reviews as positive or negative\n",
    "# http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "from ml_datasets import imdb\n",
    "# load movie reviews as a tuple (text, label)\n",
    "train_data, valid_data = imdb()\n",
    "# load a medium sized english language model in spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_docs(data):\n",
    "    \"\"\"\n",
    "    this will take a list of texts and labels \n",
    "    and transform them in spacy documents\n",
    "    \n",
    "    data: list(tuple(text, label))\n",
    "    \n",
    "    returns: List(spacy.Doc.doc)\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = []\n",
    "    # nlp.pipe([texts]) is way faster than running \n",
    "    # nlp(text) for each text\n",
    "    # as_tuples allows us to pass in a tuple, \n",
    "    # the first one is treated as text\n",
    "    # the second one will get returned as it is.\n",
    "    \n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):\n",
    "        \n",
    "        # we need to set the (text)cat(egory) for each document\n",
    "        doc.cats[\"positive\"] = label\n",
    "        \n",
    "        # put them into a nice list\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are so far only interested in the first 5000 reviews\n",
    "# this will keep the training time short.\n",
    "# In practice take as much data as you can get.\n",
    "# you can always reduce it to make the script even faster.\n",
    "num_texts = 5000\n",
    "# first we need to transform all the training data\n",
    "train_docs = make_docs(train_data[:num_texts])\n",
    "# then we save it in a binary file to disc\n",
    "doc_bin = DocBin(docs=train_docs)\n",
    "doc_bin.to_disk(\"./data/train.spacy\")\n",
    "# repeat for validation data\n",
    "valid_docs = make_docs(valid_data[:num_texts])\n",
    "doc_bin = DocBin(docs=valid_docs)\n",
    "doc_bin.to_disk(\"./data/valid.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy init fill-config ./base_config.cfg ./config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy train config.cfg --output ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# load thebest model from training\n",
    "nlp = spacy.load(\"output/model-best\")\n",
    "text = \"\"\n",
    "print(\"type : ‘quit’ to exit\")\n",
    "# predict the sentiment until someone writes quit\n",
    "while text != \"quit\":\n",
    "    text = input(\"Please enter example input: \")\n",
    "    doc = nlp(text)\n",
    "    if doc.cats['positive'] >.5:\n",
    "        print(f\"the sentiment is positive\")\n",
    "    else:\n",
    "        print(f\"the sentiment is negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXCQvEbrxRhE"
   },
   "source": [
    "### Text Classification (Example 2)\n",
    "#### The example below only works for SpaCy v3.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEGGnockjtmC"
   },
   "outputs": [],
   "source": [
    "# Import pandas & read csv file\n",
    "import pandas as pd\n",
    "reviews=pd.read_csv(\"https://raw.githubusercontent.com/hanzhang0420/Women-Clothing-E-commerce/master/Womens%20Clothing%20E-Commerce%20Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "td4dSZ5xjwe6",
    "outputId": "473166a7-8555-463c-a091-4ff88228033d"
   },
   "outputs": [],
   "source": [
    "# Extract desired columns and view the dataframe \n",
    "reviews = reviews[['Review Text','Recommended IND']].dropna()\n",
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Textcat Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQxmxbX-jz81"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "textcat = nlp.add_pipe(\"textcat\")\n",
    "textcat.add_label(\"POSITIVE\")\n",
    "textcat.add_label(\"NEGATIVE\")\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DjR2JIuk5rM",
    "outputId": "3fb311b6-459c-49c0-9ab8-1b7b8a2d4759"
   },
   "outputs": [],
   "source": [
    "# Converting the dataframe into a list of tuples\n",
    "reviews['tuples'] = reviews.apply(lambda row: (row['Review Text'],row['Recommended IND']), axis=1)\n",
    "train =reviews['tuples'].tolist()\n",
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sj-4zHWHk-10"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def load_data(limit=0, split=0.8):\n",
    "    train_data=train\n",
    "    # Shuffle the data\n",
    "    random.shuffle(train_data)\n",
    "    texts, labels = zip(*train_data)\n",
    "    # get the categories for each review\n",
    "    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n",
    "\n",
    "    # Splitting the training and evaluation data\n",
    "    split = int(len(train_data) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
    "\n",
    "n_texts=23486\n",
    "\n",
    "# Calling the load_data() function \n",
    "(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n",
    "\n",
    "# Processing the final format of training data\n",
    "train_data = list(zip(train_texts,[{'cats': cats} for cats in train_cats]))\n",
    "train_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "RHhHIm20EqMz",
    "outputId": "b0b4a883-b85f-4d78-bab7-807d4b3b0b2c"
   },
   "outputs": [],
   "source": [
    "from spacy.training import Example\n",
    "\n",
    "nlp.begin_training()\n",
    "\n",
    "examples = []\n",
    "for text, annots in train_data:\n",
    "    examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "\n",
    "#nlp.initialize(lambda: examples)\n",
    "\n",
    "for i in range(10):\n",
    "    random.shuffle(examples)\n",
    "    for batch in spacy.util.minibatch(examples, size=3):\n",
    "        nlp.update(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJT1fQGalhh-"
   },
   "outputs": [],
   "source": [
    "test_text=\"I hate this dress\"\n",
    "doc=nlp(test_text)\n",
    "doc.cats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hioyIcKB-xhm"
   },
   "source": [
    "## Topic 5 Overview of Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqsbYsMaIQhF"
   },
   "outputs": [],
   "source": [
    "!pip3 install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ER14CEQwYGct"
   },
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TkB3akPeskn"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "print(nlp.pipe_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3g3s6Vi5Y-or"
   },
   "outputs": [],
   "source": [
    "math_sent_a = \"\"\"The most common approach to turn this intuitive idea into a \"\"\"\\\n",
    "              \"\"\"precise definition is to define the derivative as a limit of \"\"\"\\\n",
    "              \"\"\"difference quotients of real numbers\"\"\"\n",
    "math_sent_b = \"\"\"The fundamental theorem of calculus relates antidifferentiation \"\"\"\\\n",
    "              \"\"\"with integration\"\"\"\n",
    "history_sent_c = \"\"\"He went on to lead the Conservatives to a record fourth \"\"\"\\\n",
    "                 \"\"\"consecutive electoral victory, winning the most votes in \"\"\"\\\n",
    "                 \"\"\"British electoral history with over 14 million votes at \"\"\"\\\n",
    "                 \"\"\"the 1992 general election, albeit with a reduced majority \"\"\"\\\n",
    "                 \"\"\"in the House of Commons\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79oChtWU-xhp"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "math_nlp_a = nlp(math_sent_a)\n",
    "math_nlp_b = nlp(math_sent_b)\n",
    "history_nlp_c = nlp(history_sent_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adNNrnia-xhp"
   },
   "outputs": [],
   "source": [
    "print(math_nlp_a[0].similarity(math_nlp_b[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQVeokTPbCxn"
   },
   "outputs": [],
   "source": [
    "print(math_nlp_a[0].similarity(history_nlp_c[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDjC1rKvbFI_"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "TRAIN_DATA = [\n",
    "    (\"He told us a very exciting adventure story.\", {\"cats\": {\"POSITIVE\": 1.0, \"NEGATIVE\": 0.0}}),\n",
    "    (\"She wrote him a long letter, but he didn't read it.\", {\"cats\": {\"POSITIVE\": 0.0, \"NEGATIVE\": 1.0}}),\n",
    "    (\"I am never at home on Sundays.\", {\"cats\": {\"POSITIVE\": 0.0, \"NEGATIVE\": 1.0}}),\n",
    "    (\"He ran out of money, so he had to stop playing poker.\", {\"cats\": {\"POSITIVE\": 0.0, \"NEGATIVE\": 1.0}}),\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "textcat = nlp.add_pipe(\"textcat\")\n",
    "textcat.add_label(\"POSITIVE\")\n",
    "textcat.add_label(\"NEGATIVE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training import Example\n",
    "\n",
    "examples = []\n",
    "for text, annots in TRAIN_DATA:\n",
    "    examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "\n",
    "optimizer = nlp.create_optimizer()   \n",
    "\n",
    "for i in range(10):\n",
    "    random.shuffle(examples)\n",
    "    for batch in spacy.util.minibatch(examples, size=3):\n",
    "        nlp.update(batch)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NICF – Natural Language Processing (NLP) with Python for Beginners - v1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
